{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook helps me have a good overview of my code and its directly corelated output.\n",
    "I will try gathering as much information as possible from \"apartment guides\" and save the content in a csv file for further data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "from bs4 import BeautifulSoup \n",
    "\n",
    "\n",
    "def addUrls(url, listUrl): \n",
    "    #open with GET method \n",
    "    resp=requests.get(url)         \n",
    "    if resp.status_code==200: \n",
    "        print(\"Successfully opened the web page\") \n",
    "        listUrl.append(url)\n",
    "    else:\n",
    "        print(\"Did not work\")\n",
    "    soup=BeautifulSoup(resp.text,'html.parser')\n",
    "    l=soup.find(\"a\",{\"class\":\"_2glka\"})   #_2glka has to be adjusted based on website; here this is Next Page\n",
    "    if (l.get('href') == \"\"):\n",
    "        print('Done adding')\n",
    "    else:                \n",
    "        addUrls(l.get('href'), listUrl) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addSubSites(url, subSiteList): \n",
    "    #open with GET method \n",
    "    resp=requests.get(url)         \n",
    "    if resp.status_code==200: \n",
    "        print(\"Successfully opened the web page\") \n",
    "        #listUrl.append(url)\n",
    "    else:\n",
    "        print(\"Did not work\")\n",
    "    soup=BeautifulSoup(resp.text,'html.parser')\n",
    "    l=soup.findAll(\"div\",{\"data-tid\":\"listing-info\"})\n",
    "    \n",
    "    for listing in l:\n",
    "        for link in listing:\n",
    "            #Look for the link on a listing-info id; opens subsite\n",
    "            if(link.get('href')):\n",
    "                print(\"###\")\n",
    "                #Checking for adds, I had one apartment corrupt my data so I am checking for dupicates\n",
    "                if ('https://www.apartmentguide.com'+link.get('href')) in subSiteList:\n",
    "                    print(\"DUPLICATE!! NOT ADDED, PROBABLY ADD\")\n",
    "                else:\n",
    "                    subSiteList.append('https://www.apartmentguide.com'+link.get('href'))\n",
    "                    print('Following site added: {}'.format(link.get('href')))\n",
    "        print('###')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.apartmentguide.com/apartments/Ohio/Dayton/'\n",
    "listUrl = []\n",
    "addUrls(url, listUrl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subSiteUrls = []\n",
    "for url in listUrl:\n",
    "    addSubSites(url, subSiteUrls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp=requests.get('https://www.apartmentguide.com/apartments/Ohio/West-Carrollton/Oakwood-Apartments/7190/')         \n",
    "if resp.status_code==200: \n",
    "    print(\"Successfully opened the web page\") \n",
    "    #listUrl.append(url)\n",
    "else:\n",
    "    print(\"Did not work\")\n",
    "soup=BeautifulSoup(resp.text,'html.parser')\n",
    "\n",
    "#amenities at the end of every break\n",
    "amen=soup.find(\"div\",{\"data-tid\":\"features\"})\n",
    "\n",
    "\n",
    "for j in amen.findAll(\"div\",{\"class\":\"_1adc7\"}):\n",
    "    for word in j.findAll(\"div\",{\"class\":\"_2gvi2\"}):\n",
    "        print(word.text+'#')\n",
    "    print('&&&#')\n",
    "\n",
    "#pets at the end of every break\n",
    "pets=soup.find(\"div\",{\"data-tid\":\"pets-section\"})\n",
    "\n",
    "for word in pets.findAll(\"div\",{\"class\":\"_2gvi2\"}):\n",
    "    print(word.text+'#')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gatherListingInfo(url, f):\n",
    "    global globCount\n",
    "    globCount = globCount + 1\n",
    "    #open with GET method \n",
    "    resp=requests.get(url)         \n",
    "    if resp.status_code==200: \n",
    "        print(\"Successfully opened the web page\") \n",
    "        #listUrl.append(url)\n",
    "    else:\n",
    "        print(\"Did not work\")\n",
    "    soup=BeautifulSoup(resp.text,'html.parser')\n",
    "    \n",
    "    l=soup.findAll(\"div\",{\"data-tag_item\":\"floor_plans\"})\n",
    "    #for every sub layer of div tags loop;\n",
    "    #loop through the general info if no sub entries are present\n",
    "    for i in l:\n",
    "        for j in i.findAll('div',{'data-tid':'name'}):\n",
    "            subList = i.findAll('div',{'data-tag_item':\"unit_title\"})\n",
    "            #Sublists are listed on website\n",
    "            if len(subList) > 0:\n",
    "                for item in subList:                    \n",
    "                    #find and print ratings for apartments\n",
    "                    l=soup.find(\"div\",{\"data-tid\":\"rating-stars\"})\n",
    "                    if l is not None:\n",
    "                        for k in l.findChildren('div'):\n",
    "                            for rating in k.findChildren('div'):\n",
    "                                print(rating.get('style')[6:9])\n",
    "                                #6:9 because the text is always a span element looking like <span>90%</span>\n",
    "                                f.write(rating.get('style')[6:9]+'#')\n",
    "                    else:\n",
    "                        print(\"No ratings\")\n",
    "                        f.write('NA#')\n",
    "                    #print and write to file -> apartment name\n",
    "                    print('\\n'+soup.find('h1',{'data-tid':'property-title'}).text)\n",
    "                    f.write(soup.find('h1',{'data-tid':'property-title'}).text+'#')\n",
    "                    #print apartment type\n",
    "                    print(j.text)\n",
    "                    f.write(j.text+'#')\n",
    "                    for g in item:\n",
    "                        mylist = []\n",
    "                        for yolo in g.findChildren('span', recursive=False):\n",
    "                            mylist.append(yolo.text)\n",
    "\n",
    "                        if len(mylist) == 0:\n",
    "                            #This serves for regex reasons later in the data preprocessing\n",
    "                            if g.get('data-tid') == \"fp-availability\":\n",
    "                                print('&'+g.text)\n",
    "                                f.write('&'+g.text+'#')\n",
    "                            else:\n",
    "                                print(g.text)\n",
    "                                f.write(g.text+'#')\n",
    "                        else:\n",
    "                            for item in mylist:\n",
    "                                print(item)\n",
    "                                f.write(item+'#')\n",
    "                    print(\"AMENITIES\")                    \n",
    "                    #amenities at the end of every break\n",
    "                    amen=soup.find(\"div\",{\"data-tid\":\"features\"})\n",
    "                    if amen is not None:\n",
    "                        for j in amen.findAll(\"div\",{\"class\":\"_1adc7\"}):\n",
    "                            for word in j.findAll(\"div\",{\"class\":\"_2gvi2\"}):                            \n",
    "                                print(word.text+'#')\n",
    "                                f.write(word.text+'#')\n",
    "                            print('&&&#')\n",
    "                            f.write('&&&#')\n",
    "                    else:\n",
    "                        print(\"No Amenitites\")\n",
    "                        f.write('NA#')\n",
    "                    #pets at the end of every break\n",
    "                    pets=soup.find(\"div\",{\"data-tid\":\"pets-section\"})\n",
    "                    if pets is not None: \n",
    "                        for word in pets.findAll(\"div\",{\"class\":\"_2gvi2\"}):\n",
    "                            print(word.text+'#')\n",
    "                            f.write(word.text+'#')\n",
    "                    print(\"BREAK\")\n",
    "                    f.write('\\n')\n",
    "            else:                \n",
    "                \n",
    "                for j in i.findAll('div',{\"data-tid\":\"floorplan-row\"}):\n",
    "                    l=soup.find(\"div\",{\"data-tid\":\"rating-stars\"})\n",
    "                    if l is not None:\n",
    "                        for k in l.findChildren('div'):\n",
    "                            for rating in k.findChildren('div'):\n",
    "                                print(rating.get('style')[6:9])\n",
    "                                f.write(rating.get('style')[6:9]+'#')\n",
    "                    else:\n",
    "                        print(\"No ratings\")\n",
    "                        f.write('NA#')\n",
    "                    print('\\n'+soup.find('h1',{'data-tid':'property-title'}).text)\n",
    "                    f.write(soup.find('h1',{'data-tid':'property-title'}).text+'#')\n",
    "                    for p in j:\n",
    "                        for prop in p:\n",
    "                            mylist = []\n",
    "                            for x in prop.findChildren('span', recursive = False):\n",
    "                                mylist.append(x.text)\n",
    "                            #span elements are empty which means bed and bath should be truncated within div tag    \n",
    "                            if len(mylist) == 0:\n",
    "                                if prop.get('data-tid') == \"fp-availability\":\n",
    "                                    print('&'+prop.text)\n",
    "                                    f.write(prop.text+'#')\n",
    "                                else:\n",
    "                                    print(prop.text)\n",
    "                                    f.write(prop.text+'#')\n",
    "\n",
    "                            else:\n",
    "                                for item in mylist:\n",
    "                                    print(item)\n",
    "                                    f.write(item+'#')                \n",
    "                    print(\"AMENITIES\")                    \n",
    "                    #amenities at the end of every break\n",
    "                    amen=soup.find(\"div\",{\"data-tid\":\"features\"})\n",
    "\n",
    "                    if amen is not None:\n",
    "                        for j in amen.findAll(\"div\",{\"class\":\"_1adc7\"}):\n",
    "                            for word in j.findAll(\"div\",{\"class\":\"_2gvi2\"}):                            \n",
    "                                print(word.text+'#')\n",
    "                                f.write(word.text+'#')\n",
    "                            print('&&&#')\n",
    "                            f.write('&&&#')\n",
    "                    else:\n",
    "                        print(\"No Amenitites\")\n",
    "                        f.write('NA#')\n",
    "\n",
    "                    #pets at the end of every break\n",
    "                    pets=soup.find(\"div\",{\"data-tid\":\"pets-section\"})\n",
    "                    if pets is not None:    \n",
    "                        for word in pets.findAll(\"div\",{\"class\":\"_2gvi2\"}):\n",
    "                            print(word.text+'#')\n",
    "                            f.write(word.text+'#')                                            \n",
    "                    print(\"BREAK\")\n",
    "                    \n",
    "                    f.write('\\n')\n",
    "        print('\\n')\n",
    "\n",
    "    print(\"\\n###Done with Website number {}###\\n\".format(globCount))          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globCount = 0\n",
    "f = open(\"finaltest.txt\", \"a\")\n",
    "for i in subSiteUrls:\n",
    "    gatherListingInfo(i,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('finaltest.txt', 'r') as infile, open('TEST.csv', 'w') as outfile:\n",
    "    stripped = (line.strip() for line in infile)\n",
    "    lines = (line.split(\"#\") for line in stripped if line)\n",
    "    writer = csv.writer(outfile)\n",
    "    writer.writerows(lines)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
